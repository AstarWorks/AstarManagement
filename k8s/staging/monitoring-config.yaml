apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
  labels:
    app: prometheus
    environment: staging
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      external_labels:
        cluster: 'aster-staging'
        environment: 'staging'

    rule_files:
      - "/etc/prometheus/rules/*.yml"

    alerting:
      alertmanagers:
        - static_configs:
            - targets:
              - alertmanager:9093

    scrape_configs:
      # Kubernetes API server
      - job_name: 'kubernetes-apiserver'
        kubernetes_sd_configs:
        - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https

      # Kubernetes nodes
      - job_name: 'kubernetes-nodes'
        kubernetes_sd_configs:
        - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics

      # Kubernetes pods
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name

      # Aster Management Backend
      - job_name: 'aster-backend'
        kubernetes_sd_configs:
        - role: pod
          namespaces:
            names:
            - aster-staging
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_label_app]
          action: keep
          regex: aster-backend
        - source_labels: [__meta_kubernetes_pod_container_port_name]
          action: keep
          regex: http
        - source_labels: [__address__]
          action: replace
          regex: ([^:]+)(?::\d+)?
          replacement: $1:8080
          target_label: __address__
        - target_label: __metrics_path__
          replacement: /actuator/prometheus
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name

      # Ingress Controller
      - job_name: 'nginx-ingress'
        kubernetes_sd_configs:
        - role: pod
          namespaces:
            names:
            - ingress-nginx
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
          action: keep
          regex: ingress-nginx
        - source_labels: [__meta_kubernetes_pod_container_port_name]
          action: keep
          regex: prometheus
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)

  recording_rules.yml: |
    groups:
    - name: aster_backend_rules
      rules:
      - record: aster:backend:request_rate
        expr: rate(http_requests_total{job="aster-backend"}[5m])
      
      - record: aster:backend:error_rate
        expr: rate(http_requests_total{job="aster-backend",status=~"5.."}[5m])
      
      - record: aster:backend:response_time_p95
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="aster-backend"}[5m]))
      
      - record: aster:backend:cpu_usage
        expr: rate(container_cpu_usage_seconds_total{pod=~"aster-backend-.*"}[5m])
      
      - record: aster:backend:memory_usage
        expr: container_memory_working_set_bytes{pod=~"aster-backend-.*"}

  alerting_rules.yml: |
    groups:
    - name: aster_alerts
      rules:
      - alert: HighErrorRate
        expr: aster:backend:error_rate > 0.1
        for: 5m
        labels:
          severity: warning
          service: aster-backend
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors per second"

      - alert: HighResponseTime
        expr: aster:backend:response_time_p95 > 2
        for: 5m
        labels:
          severity: warning
          service: aster-backend
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }} seconds"

      - alert: HighCPUUsage
        expr: aster:backend:cpu_usage > 0.8
        for: 10m
        labels:
          severity: warning
          service: aster-backend
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is {{ $value }}%"

      - alert: HighMemoryUsage
        expr: aster:backend:memory_usage > 1e9  # 1GB
        for: 10m
        labels:
          severity: warning
          service: aster-backend
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is {{ $value | humanize }}B"

      - alert: PodNotReady
        expr: kube_pod_status_ready{condition="false", namespace="aster-staging"} == 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Pod not ready"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is not ready"

      - alert: DeploymentReplicasMismatch
        expr: kube_deployment_status_replicas_available != kube_deployment_spec_replicas
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Deployment replicas mismatch"
          description: "Deployment {{ $labels.deployment }} has {{ $labels.replicas_available }} available replicas out of {{ $labels.replicas }} desired"

      - alert: PersistentVolumeUsageHigh
        expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) > 0.85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Persistent volume usage high"
          description: "PV usage is {{ $value | humanizePercentage }}"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
  namespace: monitoring
  labels:
    app: grafana
    environment: staging
data:
  aster-overview.json: |
    {
      "dashboard": {
        "id": null,
        "title": "Aster Management - Overview",
        "tags": ["aster", "staging"],
        "style": "dark",
        "timezone": "browser",
        "panels": [
          {
            "id": 1,
            "title": "Request Rate",
            "type": "stat",
            "targets": [
              {
                "expr": "sum(aster:backend:request_rate)",
                "legendFormat": "Requests/sec"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
          },
          {
            "id": 2,
            "title": "Error Rate",
            "type": "stat",
            "targets": [
              {
                "expr": "sum(aster:backend:error_rate)",
                "legendFormat": "Errors/sec"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
          },
          {
            "id": 3,
            "title": "Response Time (95th percentile)",
            "type": "graph",
            "targets": [
              {
                "expr": "aster:backend:response_time_p95",
                "legendFormat": "Response Time"
              }
            ],
            "gridPos": {"h": 8, "w": 24, "x": 0, "y": 8}
          },
          {
            "id": 4,
            "title": "CPU Usage",
            "type": "graph",
            "targets": [
              {
                "expr": "aster:backend:cpu_usage",
                "legendFormat": "{{ pod }}"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 16}
          },
          {
            "id": 5,
            "title": "Memory Usage",
            "type": "graph",
            "targets": [
              {
                "expr": "aster:backend:memory_usage",
                "legendFormat": "{{ pod }}"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 16}
          }
        ],
        "time": {"from": "now-1h", "to": "now"},
        "refresh": "30s"
      }
    }
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
  labels:
    app: alertmanager
    environment: staging
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'localhost:587'
      smtp_from: 'alerts@aster-management.example.com'

    route:
      group_by: ['alertname']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'web.hook'

    receivers:
    - name: 'web.hook'
      webhook_configs:
      - url: 'http://localhost:5001/'
        send_resolved: true

    # Example Slack configuration (uncomment and configure)
    # - name: 'slack-alerts'
    #   slack_configs:
    #   - api_url: 'YOUR_SLACK_WEBHOOK_URL'
    #     channel: '#alerts'
    #     title: 'Aster Management Alert'
    #     text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

    inhibit_rules:
      - source_match:
          severity: 'critical'
        target_match:
          severity: 'warning'
        equal: ['alertname', 'dev', 'instance']